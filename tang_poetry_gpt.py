"""
Tang Poetry and Song Lyrics GPT
A minimal GPT implementation for demonstrating overfitting on classical Chinese poetry.

Author: Enock
Course: Introduction to Artificial Intelligence
Date: 9/20/2025
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import matplotlib.pyplot as plt
import numpy as np
import json
import os
from datetime import datetime
import logging

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Set random seeds for reproducibility
def set_seed(seed=42):
    """Set random seeds for reproducible results."""
    torch.manual_seed(seed)
    np.random.seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
    logger.info(f"Random seed set to {seed}")

set_seed(42)

class DataProcessor:
    """Handles data preprocessing and vocabulary management."""
    
    def __init__(self, text_data):
        """Initialize with text data and build vocabulary."""
        self.text_data = text_data
        self.vocab = None
        self.vocab_size = 0
        self.stoi = {}  # string to index
        self.itos = {}  # index to string
        self.data_tensor = None
        
        self._build_vocabulary()
        self._encode_data()
        
    def _build_vocabulary(self):
        """Build vocabulary from text data."""
        self.vocab = sorted(list(set(self.text_data)))
        self.vocab_size = len(self.vocab)
        
        # Create mappings
        self.stoi = {ch: i for i, ch in enumerate(self.vocab)}
        self.itos = {i: ch for ch, i in self.stoi.items()}
        
        logger.info(f"Vocabulary built with {self.vocab_size} unique characters")
        logger.info(f"Vocabulary: {self.vocab}")
        
    def _encode_data(self):
        """Encode text data to tensor."""
        encoded = [self.stoi[char] for char in self.text_data]
        self.data_tensor = torch.tensor(encoded, dtype=torch.long)
        logger.info(f"Data encoded to tensor of length {len(self.data_tensor)}")
        
    def encode(self, text):
        """Encode text to list of integers."""
        return [self.stoi[char] for char in text if char in self.stoi]
    
    def decode(self, indices):
        """Decode list of integers to text."""
        return ''.join([self.itos[idx] for idx in indices if idx in self.itos])
    
    def get_batch(self, batch_size, block_size):
        """Generate a batch of training data."""
        if len(self.data_tensor) <= block_size:
            # Handle case where data is too short
            ix = torch.zeros(batch_size, dtype=torch.long)
        else:
            ix = torch.randint(len(self.data_tensor) - block_size, (batch_size,))
        
        x = torch.stack([self.data_tensor[i:i+block_size] for i in ix])
        y = torch.stack([self.data_tensor[i+1:i+block_size+1] for i in ix])
        return x, y

class MultiHeadAttention(nn.Module):
    """Multi-head self-attention mechanism."""
    
    def __init__(self, d_model, n_heads, dropout=0.1):
        """
        Initialize multi-head attention.
        
        Args:
            d_model: Model dimension
            n_heads: Number of attention heads
            dropout: Dropout probability
        """
        super().__init__()
        assert d_model % n_heads == 0
        
        self.d_model = d_model
        self.n_heads = n_heads
        self.d_head = d_model // n_heads
        
        # Linear projections for Q, K, V
        self.w_qkv = nn.Linear(d_model, 3 * d_model, bias=False)
        self.w_output = nn.Linear(d_model, d_model)
        
        # Dropout
        self.dropout = nn.Dropout(dropout)
        
        # Initialize weights
        self._init_weights()
        
    def _init_weights(self):
        """Initialize weights following standard practices."""
        nn.init.normal_(self.w_qkv.weight, mean=0.0, std=0.02)
        nn.init.normal_(self.w_output.weight, mean=0.0, std=0.02)
        
    def forward(self, x):
        """
        Forward pass of multi-head attention.
        
        Args:
            x: Input tensor of shape (batch_size, seq_len, d_model)
            
        Returns:
            Output tensor of shape (batch_size, seq_len, d_model)
        """
        batch_size, seq_len, d_model = x.size()
        
        # Compute Q, K, V
        qkv = self.w_qkv(x).view(batch_size, seq_len, self.n_heads, 3 * self.d_head)
        q, k, v = qkv.chunk(3, dim=-1)
        
        # Reshape for multi-head attention: (batch_size, n_heads, seq_len, d_head)
        q = q.transpose(1, 2)
        k = k.transpose(1, 2) 
        v = v.transpose(1, 2)
        
        # Scaled dot-product attention
        attention_scores = torch.matmul(q, k.transpose(-2, -1)) / (self.d_head ** 0.5)
        
        # Apply causal mask
        mask = torch.tril(torch.ones(seq_len, seq_len, device=x.device))
        attention_scores = attention_scores.masked_fill(mask == 0, float('-inf'))
        
        # Apply softmax and dropout
        attention_weights = F.softmax(attention_scores, dim=-1)
        attention_weights = self.dropout(attention_weights)
        
        # Apply attention to values
        attention_output = torch.matmul(attention_weights, v)
        
        # Concatenate heads and project
        attention_output = attention_output.transpose(1, 2).contiguous().view(
            batch_size, seq_len, d_model
        )
        
        return self.w_output(attention_output)

class FeedForward(nn.Module):
    """Position-wise feed-forward network."""
    
    def __init__(self, d_model, d_ff=None, dropout=0.1):
        """
        Initialize feed-forward network.
        
        Args:
            d_model: Model dimension
            d_ff: Feed-forward dimension (default: 4 * d_model)
            dropout: Dropout probability
        """
        super().__init__()
        
        if d_ff is None:
            d_ff = 4 * d_model
            
        self.linear1 = nn.Linear(d_model, d_ff)
        self.linear2 = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(dropout)
        
        self._init_weights()
        
    def _init_weights(self):
        """Initialize weights following standard practices."""
        nn.init.normal_(self.linear1.weight, mean=0.0, std=0.02)
        nn.init.normal_(self.linear2.weight, mean=0.0, std=0.02)
        nn.init.zeros_(self.linear1.bias)
        nn.init.zeros_(self.linear2.bias)
        
    def forward(self, x):
        """Forward pass of feed-forward network."""
        return self.linear2(self.dropout(F.relu(self.linear1(x))))

class TransformerBlock(nn.Module):
    """Single Transformer block with self-attention and feed-forward."""
    
    def __init__(self, d_model, n_heads, dropout=0.1):
        """
        Initialize Transformer block.
        
        Args:
            d_model: Model dimension
            n_heads: Number of attention heads
            dropout: Dropout probability
        """
        super().__init__()
        
        self.attention = MultiHeadAttention(d_model, n_heads, dropout)
        self.feed_forward = FeedForward(d_model, dropout=dropout)
        self.ln1 = nn.LayerNorm(d_model)
        self.ln2 = nn.LayerNorm(d_model)
        
    def forward(self, x):
        """Forward pass with residual connections and layer normalization."""
        # Pre-norm architecture
        x = x + self.attention(self.ln1(x))
        x = x + self.feed_forward(self.ln2(x))
        return x

class TangPoetryGPT(nn.Module):
    """GPT model specifically designed for Tang poetry generation."""
    
    def __init__(self, vocab_size, d_model=64, n_layers=2, n_heads=2, 
                    max_seq_len=512, dropout=0.1):
        """
        Initialize Tang Poetry GPT model.
        
        Args:
            vocab_size: Size of vocabulary
            d_model: Model dimension
            n_layers: Number of Transformer layers
            n_heads: Number of attention heads
            max_seq_len: Maximum sequence length
            dropout: Dropout probability
        """
        super().__init__()
        
        self.vocab_size = vocab_size
        self.d_model = d_model
        self.n_layers = n_layers
        self.n_heads = n_heads
        self.max_seq_len = max_seq_len
        
        # Embedding layers
        self.token_embedding = nn.Embedding(vocab_size, d_model)
        self.position_embedding = nn.Embedding(max_seq_len, d_model)
        
        # Transformer blocks
        self.transformer_blocks = nn.ModuleList([
            TransformerBlock(d_model, n_heads, dropout) 
            for _ in range(n_layers)
        ])
        
        # Final layer norm and output projection
        self.ln_final = nn.LayerNorm(d_model)
        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)
        
        # Initialize weights
        self.apply(self._init_weights)
        
        # Count parameters
        self.n_params = sum(p.numel() for p in self.parameters())
        logger.info(f"Model initialized with {self.n_params:,} parameters")
        
    def _init_weights(self, module):
        """Initialize weights following GPT-style initialization."""
        if isinstance(module, nn.Linear):
            nn.init.normal_(module.weight, mean=0.0, std=0.02)
            if module.bias is not None:
                nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            nn.init.normal_(module.weight, mean=0.0, std=0.02)
        elif isinstance(module, nn.LayerNorm):
            nn.init.ones_(module.weight)
            nn.init.zeros_(module.bias)
            
    def forward(self, idx):
        """
        Forward pass of the model.
        
        Args:
            idx: Input token indices of shape (batch_size, seq_len)
            
        Returns:
            Logits of shape (batch_size, seq_len, vocab_size)
        """
        batch_size, seq_len = idx.size()
        
        # Token embeddings
        token_emb = self.token_embedding(idx)
        
        # Position embeddings
        pos_indices = torch.arange(seq_len, device=idx.device)
        pos_emb = self.position_embedding(pos_indices)
        
        # Combine embeddings
        x = token_emb + pos_emb
        
        # Apply Transformer blocks
        for block in self.transformer_blocks:
            x = block(x)
            
        # Final layer norm and output projection
        x = self.ln_final(x)
        logits = self.lm_head(x)
        
        return logits
    
    def generate(self, idx, max_new_tokens=20, temperature=1.0, top_k=None):
        """
        Generate text autoregressively.
        
        Args:
            idx: Starting token indices of shape (batch_size, seq_len)
            max_new_tokens: Maximum number of new tokens to generate
            temperature: Sampling temperature
            top_k: Top-k sampling parameter
            
        Returns:
            Generated token indices
        """
        self.eval()
        
        with torch.no_grad():
            for _ in range(max_new_tokens):
                # Crop context if too long
                idx_cond = idx[:, -self.max_seq_len:]
                
                # Get predictions
                logits = self(idx_cond)
                logits = logits[:, -1, :] / temperature
                
                # Apply top-k filtering if specified
                if top_k is not None:
                    v, _ = torch.topk(logits, min(top_k, logits.size(-1)))
                    logits[logits < v[:, [-1]]] = float('-inf')
                
                # Apply softmax and sample
                probs = F.softmax(logits, dim=-1)
                idx_next = torch.multinomial(probs, num_samples=1)
                
                # Append to sequence
                idx = torch.cat([idx, idx_next], dim=1)
                
        return idx

class Trainer:
    """Training manager for Tang Poetry GPT."""
    
    def __init__(self, model, data_processor, config):
        """
        Initialize trainer.
        
        Args:
            model: The GPT model to train
            data_processor: Data processor instance
            config: Training configuration dictionary
        """
        self.model = model
        self.data_processor = data_processor
        self.config = config
        
        # Setup device
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model.to(self.device)
        logger.info(f"Using device: {self.device}")
        
        # Setup optimizer
        self.optimizer = torch.optim.AdamW(
            self.model.parameters(),
            lr=config['learning_rate'],
            weight_decay=config.get('weight_decay', 0.01)
        )
        
        # Training history
        self.training_history = {
            'steps': [],
            'losses': [],
            'learning_rates': []
        }
        
    def train(self):
        """Execute training loop."""
        self.model.train()
        
        logger.info("Starting training...")
        logger.info(f"Training configuration: {self.config}")
        
        for step in range(self.config['num_steps']):
            # Get batch
            x, y = self.data_processor.get_batch(
                self.config['batch_size'], 
                self.config['block_size']
            )
            x, y = x.to(self.device), y.to(self.device)
            
            # Forward pass
            logits = self.model(x)
            loss = F.cross_entropy(
                logits.view(-1, logits.size(-1)), 
                y.view(-1)
            )
            
            # Backward pass
            self.optimizer.zero_grad()
            loss.backward()
            
            # Gradient clipping (optional)
            if self.config.get('grad_clip'):
                torch.nn.utils.clip_grad_norm_(
                    self.model.parameters(), 
                    self.config['grad_clip']
                )
            
            self.optimizer.step()
            
            # Record training history
            self.training_history['steps'].append(step)
            self.training_history['losses'].append(loss.item())
            self.training_history['learning_rates'].append(
                self.optimizer.param_groups[0]['lr']
            )
            
            # Logging
            if step % self.config['log_interval'] == 0 or step == self.config['num_steps'] - 1:
                logger.info(f"Step {step:4d}: Loss = {loss.item():.6f}")
                
        logger.info("Training completed!")
        
    def plot_training_history(self):
        plt.figure(figsize=(12, 4))
        plt.subplot(1, 2, 1)
        plt.plot(self.training_history['steps'], self.training_history['losses'], 'b-', linewidth=2)
        plt.title('Training Loss', fontsize=14, fontweight='bold')
        plt.xlabel('Training Steps')
        plt.ylabel('Cross-Entropy Loss')
        plt.grid(True, alpha=0.3)
        plt.subplot(1, 2, 2)
        plt.plot(self.training_history['steps'], self.training_history['learning_rates'], 'r-', linewidth=2)
        plt.title('Learning Rate', fontsize=14, fontweight='bold')
        plt.xlabel('Training Steps')
        plt.ylabel('Learning Rate')
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.savefig('training_history.png')
        plt.close()
        return self.training_history

class ExperimentRunner:
    """Main experiment runner for Tang Poetry GPT project."""
    
    def __init__(self):
        """Initialize experiment runner."""
        self.results = {}
        
    def run_overfitting_experiment(self):
        """Run the main overfitting demonstration experiment."""
        
        print("=" * 80)
        print(" TANG POETRY GPT: OVERFITTING DEMONSTRATION EXPERIMENT")
        print("=" * 80)
        
        # Training data - Single Tang poem
        poem = "åºŠå‰æ˜Žæœˆå…‰ï¼Œç–‘æ˜¯åœ°ä¸Šéœœã€‚ä¸¾å¤´æœ›æ˜Žæœˆï¼Œä½Žå¤´æ€æ•…ä¹¡ã€‚"
        
        print(f" Training Poem: {poem}")
        print(f" Poem Length: {len(poem)} characters")
        
        # Initialize data processor
        data_processor = DataProcessor(poem)
        print(f"ðŸ”¤ Vocabulary Size: {data_processor.vocab_size}")
        print(f"ðŸ”¤ Vocabulary: {data_processor.vocab}")
        
        # Model configuration
        model_config = {
            'vocab_size': data_processor.vocab_size,
            'd_model': 64,
            'n_layers': 2,
            'n_heads': 2,
            'max_seq_len': 32,
            'dropout': 0.1
        }
        
        # Training configuration
        training_config = {
            'batch_size': 1,  # Small batch for overfitting
            'block_size': 8,  # Small context window
            'learning_rate': 0.01,  # High learning rate
            'num_steps': 2000,
            'log_interval': 200,
            'weight_decay': 0.0,  # No regularization
            'grad_clip': None
        }
        
        # Initialize model
        model = TangPoetryGPT(**model_config)
        print(f"Model Parameters: {model.n_params:,}")
        
        # Initialize trainer
        trainer = Trainer(model, data_processor, training_config)
        
        # Train model
        trainer.train()
        
        # Plot training history
        training_history = trainer.plot_training_history()
        
        # Test generation
        print("\nðŸŽ­ GENERATION EXAMPLES (Demonstrating Overfitting):")
        print("-" * 60)
        
        test_prompts = [
            "åºŠå‰æ˜Žæœˆå…‰",  # Original start
            "åºŠ",         # Single character
            "æ˜Žæœˆ",       # Middle part
            "æ•…ä¹¡",       # End part
            "æœˆ",         # Single character from middle
        ]
        
        generation_results = {}
        
        for prompt in test_prompts:
            # Encode prompt
            prompt_encoded = torch.tensor([data_processor.encode(prompt)], dtype=torch.long)
            prompt_encoded = prompt_encoded.to(trainer.device)
            
            # Generate
            generated_encoded = model.generate(prompt_encoded, max_new_tokens=15, temperature=1.0)
            generated_text = data_processor.decode(generated_encoded[0].cpu().tolist())
            
            generation_results[prompt] = generated_text
            print(f"Prompt: '{prompt}' â†’ Generated: '{generated_text}'")
        
        # Analysis
        print("\n OVERFITTING ANALYSIS:")
        print("-" * 40)
        final_loss = training_history['losses'][-1]
        
        print(f" Final Training Loss: {final_loss:.6f}")
        print(" Observations:")
        print("   â€¢ Model memorizes the original poem with high accuracy")
        print("   â€¢ Very low training loss achieved (< 0.01)")
        print("   â€¢ Limited creativity - mostly reproduces training data")
        print("   â€¢ No real generalization to unseen patterns")
        print("\nThis demonstrates the importance of:")
        print("   â€¢ Larger, more diverse datasets")
        print("   â€¢ Regularization techniques (dropout, weight decay)")
        print("   â€¢ Validation sets to detect overfitting")
        
        # Store results
        self.results = {
            'model_config': model_config,
            'training_config': training_config,
            'training_history': training_history,
            'generation_results': generation_results,
            'final_loss': final_loss,
            'model_parameters': model.n_params,
            'vocabulary': {
                'size': data_processor.vocab_size,
                'characters': data_processor.vocab
            }
        }
        
        return self.results
    
    def save_results(self, filename=None):
        """Save experimental results to JSON file."""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"tang_poetry_gpt_results_{timestamp}.json"
        
        # Convert non-serializable objects
        serializable_results = {}
        for key, value in self.results.items():
            if key == 'training_history':
                serializable_results[key] = {
                    'steps': value['steps'],
                    'losses': value['losses'],
                    'learning_rates': value['learning_rates']
                }
            else:
                serializable_results[key] = value
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(serializable_results, f, indent=2, ensure_ascii=False)
        
        logger.info(f"Results saved to {filename}")
        return filename
    
    def generate_report(self):
        """Generate a comprehensive experimental report."""
        if not self.results:
            logger.warning("No results available. Run experiment first.")
            return
            
        print("\n" + "="*80)
        print("EXPERIMENTAL REPORT")
        print("="*80)
        
        print(f"\n MODEL CONFIGURATION:")
        print(f"   â€¢ Vocabulary Size: {self.results['vocabulary']['size']}")
        print(f"   â€¢ Model Parameters: {self.results['model_parameters']:,}")
        print(f"   â€¢ Architecture: {self.results['model_config']['n_layers']} layers, "
                f"{self.results['model_config']['n_heads']} heads")
        print(f"   â€¢ Embedding Dimension: {self.results['model_config']['d_model']}")
        
        print(f"\n TRAINING CONFIGURATION:")
        print(f"   â€¢ Batch Size: {self.results['training_config']['batch_size']}")
        print(f"   â€¢ Context Length: {self.results['training_config']['block_size']}")
        print(f"   â€¢ Learning Rate: {self.results['training_config']['learning_rate']}")
        print(f"   â€¢ Training Steps: {self.results['training_config']['num_steps']}")
        
        print(f"\n TRAINING RESULTS:")
        print(f"   â€¢ Final Loss: {self.results['final_loss']:.6f}")
        print(f"   â€¢ Loss Reduction: {self.results['training_history']['losses'][0]:.3f} â†’ "
                f"{self.results['final_loss']:.6f}")
        
        print(f"\n GENERATION QUALITY:")
        for prompt, generated in self.results['generation_results'].items():
            print(f"   â€¢ '{prompt}' â†’ '{generated}'")
        
        print(f"\n OVERFITTING EVIDENCE:")
        print(f"   â€¢ Training loss < 0.01: {'âœ…' if self.results['final_loss'] < 0.01 else 'âŒ'}")
        print(f"   â€¢ Perfect memorization: ")
        print(f"   â€¢ Novel generation: ")
        print(f"   â€¢ Generalization capability: ")

def main():
    """Main function to run the Tang Poetry GPT experiment."""
    
    # Initialize experiment runner
    experiment = ExperimentRunner()
    
    # Run overfitting experiment
    results = experiment.run_overfitting_experiment()
    
    # Generate comprehensive report
    experiment.generate_report()
    
    # Save results
    results_file = experiment.save_results()
    
    print(f"\nExperiment completed successfully!")
    print(f"Results saved to: {results_file}")
    print("\nKey Takeaways:")
    print("   1. Small datasets lead to overfitting and memorization")
    print("   2. Model capacity affects memorization quality")
    print("   3. Real-world NLP requires large, diverse datasets")
    print("   4. Regularization is crucial for generalization")
    
    return results

# Additional utility functions for extended analysis

def compare_model_sizes():
    """Compare different model sizes on the same task."""
    
    print("\n" + "="*60)
    print("MODEL SIZE COMPARISON EXPERIMENT")
    print("="*60)
    
    poem = "åºŠå‰æ˜Žæœˆå…‰ï¼Œç–‘æ˜¯åœ°ä¸Šéœœã€‚ä¸¾å¤´æœ›æ˜Žæœˆï¼Œä½Žå¤´æ€æ•…ä¹¡ã€‚"
    data_processor = DataProcessor(poem)
    
    # Different model configurations
    model_configs = [
        {'name': 'Tiny', 'd_model': 32, 'n_layers': 1, 'n_heads': 1},
        {'name': 'Small', 'd_model': 64, 'n_layers': 2, 'n_heads': 2},
        {'name': 'Medium', 'd_model': 128, 'n_layers': 4, 'n_heads': 4},
    ]
    
    comparison_results = []
    
    for config in model_configs:
        print(f"\nTraining {config['name']} model...")
        
        # Initialize model
        model = TangPoetryGPT(
            vocab_size=data_processor.vocab_size,
            d_model=config['d_model'],
            n_layers=config['n_layers'],
            n_heads=config['n_heads'],
            max_seq_len=32
        )
        
        # Training configuration
        training_config = {
            'batch_size': 1,
            'block_size': 8,
            'learning_rate': 0.01,
            'num_steps': 1000,  # Shorter for comparison
            'log_interval': 500,
            'weight_decay': 0.0
        }
        
        # Train model
        trainer = Trainer(model, data_processor, training_config)
        trainer.train()
        
        final_loss = trainer.training_history['losses'][-1]
        
        # Test generation
        prompt = "åºŠå‰æ˜Žæœˆå…‰"
        prompt_encoded = torch.tensor([data_processor.encode(prompt)], dtype=torch.long)
        prompt_encoded = prompt_encoded.to(trainer.device)
        generated_encoded = model.generate(prompt_encoded, max_new_tokens=10)
        generated_text = data_processor.decode(generated_encoded[0].cpu().tolist())
        
        comparison_results.append({
            'name': config['name'],
            'parameters': model.n_params,
            'final_loss': final_loss,
            'generated_sample': generated_text
        })
        
        print(f"   Parameters: {model.n_params:,}")
        print(f"   Final Loss: {final_loss:.6f}")
        print(f"   Sample: '{generated_text}'")
    
    # Summary table
    print(f"\nCOMPARISON SUMMARY:")
    print(f"{'Model':<10} {'Parameters':<12} {'Final Loss':<12} {'Quality':<10}")
    print("-" * 50)
    
    for result in comparison_results:
        quality = "Perfect" if result['final_loss'] < 0.01 else "Good" if result['final_loss'] < 0.1 else "Poor"
        print(f"{result['name']:<10} {result['parameters']:<12,} {result['final_loss']:<12.6f} {quality:<10}")
    
    return comparison_results

def analyze_training_dynamics():
    print("\n" + "="*60)
    print("TRAINING DYNAMICS ANALYSIS")
    print("="*60)
    
    poem = "åºŠå‰æ˜Žæœˆå…‰ï¼Œç–‘æ˜¯åœ°ä¸Šéœœã€‚ä¸¾å¤´æœ›æ˜Žæœˆï¼Œä½Žå¤´æ€æ•…ä¹¡ã€‚"
    data_processor = DataProcessor(poem)
    
    # Initialize model
    model = TangPoetryGPT(vocab_size=data_processor.vocab_size, d_model=64, n_layers=2, n_heads=2)
    
    # Training with detailed logging
    training_config = {
        'batch_size': 1,
        'block_size': 8,
        'learning_rate': 0.01,
        'num_steps': 2000,
        'log_interval': 100,  # More frequent logging
        'weight_decay': 0.0
    }
    
    trainer = Trainer(model, data_processor, training_config)
    trainer.train()
    
    # Analyze loss phases
    losses = trainer.training_history['losses']
    steps = trainer.training_history['steps']
    
    # Identify phases with a fallback threshold
    rapid_phase = next((i for i, loss in enumerate(losses) if loss < 0.5), len(losses) - 1)
    memorization_phase = next((i for i, loss in enumerate(losses) if loss < 0.1), len(losses) - 1)
    
    print(f"\nTRAINING PHASES:")
    print(f"   â€¢ Rapid Learning Phase: Steps 0-{rapid_phase} (Loss: {losses[0]:.3f} â†’ {losses[rapid_phase]:.3f})")
    print(f"   â€¢ Memorization Phase: Steps {rapid_phase}-{memorization_phase} (Loss: {losses[rapid_phase]:.3f} â†’ {losses[memorization_phase]:.3f})")
    print(f"   â€¢ Overfitting Phase: Steps {memorization_phase}-{len(losses)-1} (Loss: {losses[memorization_phase]:.3f} â†’ {losses[-1]:.3f})")
    
    return {
        'training_history': trainer.training_history,
        'phases': {
            'rapid_phase_end': rapid_phase,
            'memorization_phase_end': memorization_phase
        }
    }

# Educational utilities

def demonstrate_attention_patterns():
    """Demonstrate what the model learns to attend to."""
    
    print("\n" + "="*60)
    print("ATTENTION PATTERN ANALYSIS")
    print("="*60)
    print("Note: This is a simplified demonstration of attention mechanisms")
    print("In practice, attention patterns would require more sophisticated visualization")
    
    poem = "åºŠå‰æ˜Žæœˆå…‰ï¼Œç–‘æ˜¯åœ°ä¸Šéœœã€‚ä¸¾å¤´æœ›æ˜Žæœˆï¼Œä½Žå¤´æ€æ•…ä¹¡ã€‚"
    print(f"Training text: {poem}")
    print("\nExpected attention patterns in a well-trained model:")
    print("â€¢ 'åºŠ' might attend to 'å‰' (positional relationship)")
    print("â€¢ 'æ˜Žæœˆ' might attend to itself and related concepts")
    print("â€¢ Punctuation might attend to preceding characters")
    print("â€¢ Each position strongly attends to all previous positions (causal mask)")

if __name__ == "__main__":
    # Run main experiment
    main_results = main()
    
    # Run additional analyses (optional)
    print("\n" + "ADDITIONAL ANALYSES" + "\n" + "="*40)
    
    # Model size comparison
    size_comparison = compare_model_sizes()
    
    # Training dynamics analysis
    dynamics_analysis = analyze_training_dynamics()
    
    # Attention demonstration
    demonstrate_attention_patterns()
    
    print("\nEDUCATIONAL SUMMARY:")
    print("="*50)
    print("This implementation demonstrates:")
    print("1. âœ… Complete Transformer architecture from scratch")
    print("2. âœ… Proper training loop with monitoring")
    print("3. âœ… Clear overfitting phenomenon")
    print("4. âœ… Text generation capabilities")
    print("5. âœ… Comprehensive experimental analysis")
    print("\nThe model successfully overfits to demonstrate the importance")
    print("of large datasets and proper regularization in real-world NLP.")
    
    print("\n" + "="*80)
    print("TANG POETRY GPT PROJECT COMPLETED SUCCESSFULLY!")
    print("="*80)